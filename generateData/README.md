# üè¶ Banking Data Pipeline: PySpark + DBT

A complete, beginner-friendly banking data pipeline that generates realistic transaction data with fraud detection, credit risk assessment, and customer analytics.

---

## üìñ Table of Contents

1. [What is This Project?](#what-is-this-project)
2. [Understanding the Technologies](#understanding-the-technologies)
3. [How PySpark Generates Data](#how-pyspark-generates-data)
4. [What DBT Does For You](#what-dbt-does-for-you)
5. [The Star Schema Pattern](#the-star-schema-pattern)
6. [Quick Start Guide](#quick-start-guide)
7. [Project Structure](#project-structure)
8. [Sample Queries](#sample-queries)
9. [Troubleshooting](#troubleshooting)

---

## üéØ What is This Project?

This project simulates a **complete banking system** with realistic data. Think of it as building a mini-bank's data warehouse from scratch!

### What You Get:
- **Fake but realistic banking data** - Customers, accounts, transactions, loans, cards
- **A data warehouse** - Organized data ready for analysis
- **Analytics reports** - Fraud detection, customer segmentation, risk assessment

### Real-World Use Cases:
- üîç **Fraud Detection** - Find suspicious transactions
- üë• **Customer Segmentation** - Group customers by behavior
- üí≥ **Credit Risk** - Assess loan default probability
- üìä **Business Intelligence** - Branch performance, transaction trends

---

## üß† Understanding the Technologies

### What is PySpark?

**PySpark** is Python's interface to Apache Spark - a powerful tool for processing large amounts of data.

```
Think of it like this:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Python (easy to write) + Spark (fast processing)      ‚îÇ
‚îÇ                        =                                ‚îÇ
‚îÇ              PySpark (best of both!)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Why use PySpark?**
- ‚ö° **Speed**: Can process millions of rows in seconds
- üìà **Scalability**: Same code works on your laptop or a cluster of 1000 servers
- üîß **Features**: Built-in functions for data manipulation, SQL support

### What is DBT (Data Build Tool)?

**DBT** transforms raw data into analysis-ready tables using SQL. It's like having a smart assistant that organizes your messy data.

```
Without DBT:                          With DBT:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Raw Data    ‚îÇ                     ‚îÇ  Raw Data    ‚îÇ
‚îÇ  (messy)     ‚îÇ                     ‚îÇ  (messy)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                                    ‚îÇ
       ‚îÇ Write complex                      ‚îÇ Write simple
       ‚îÇ Python scripts                     ‚îÇ SQL files
       ‚îÇ                                    ‚îÇ
       ‚ñº                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Clean Data  ‚îÇ                     ‚îÇ  Clean Data  ‚îÇ ‚Üê DBT handles:
‚îÇ  (manual     ‚îÇ                     ‚îÇ  (automatic  ‚îÇ   ‚Ä¢ Dependencies
‚îÇ   work)      ‚îÇ                     ‚îÇ   magic!)    ‚îÇ   ‚Ä¢ Testing
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚Ä¢ Documentation
```

**What DBT gives you:**
1. **Organization** - Separates raw, staging, and final data
2. **Dependencies** - Knows which tables need to be built first
3. **Testing** - Validates your data automatically
4. **Documentation** - Auto-generates data documentation
5. **Lineage** - Shows where each piece of data comes from

### What is PostgreSQL?

**PostgreSQL** is where all your data lives - a powerful, free database.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PostgreSQL Database                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇraw_customers‚îÇ  ‚îÇraw_accounts ‚îÇ  ‚îÇraw_transact ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ         ‚îÇ               ‚îÇ                ‚îÇ              ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                         ‚ñº                               ‚îÇ
‚îÇ                    DBT transforms                       ‚îÇ
‚îÇ                         ‚îÇ                               ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ         ‚ñº               ‚ñº               ‚ñº              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇdim_customers‚îÇ  ‚îÇfct_transact ‚îÇ  ‚îÇanalytics    ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚öôÔ∏è How PySpark Generates Data

### The Magic Behind Synthetic Data

We use a library called **dbldatagen** (Databricks Labs Data Generator) to create realistic fake data.

### Step-by-Step Process:

#### 1. Define the Data Structure

```python
# In banking_generator.py, we define what data looks like:
customer_spec = (
    dg.DataGenerator(
        spark,                    # Use Spark for processing
        name="customers",         # Name this data generator
        rows=5000,               # Create 5000 customers
        partitions=8,            # Split work across 8 workers
    )
    .withIdOutput()              # Add an ID column
    .withColumn(
        "customer_id",           # Column name
        StringType(),            # Data type (text)
        format="CUST%08d",       # Format: CUST00000001, CUST00000002...
        baseColumn="id"          # Base it on the ID
    )
    .withColumn(
        "first_name",
        StringType(),
        template=r"\\w",         # Generate random word (name)
        random=True
    )
    .withColumn(
        "credit_score",
        IntegerType(),           # Whole number
        minValue=300,            # Minimum credit score
        maxValue=850,            # Maximum credit score
        random=True              # Random value in range
    )
)
```

#### 2. Build and Save the Data

```python
# Generate the DataFrame (table)
df = customer_spec.build()

# Write to PostgreSQL
df.write.format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/device_db") \
    .option("dbtable", "raw_customers") \
    .option("user", "postgres") \
    .option("password", "postgres") \
    .mode("overwrite") \
    .save()
```

### Data Generation Techniques Used:

| Technique | Example | Use Case |
|-----------|---------|----------|
| **Format strings** | `"CUST%08d"` ‚Üí CUST00000001 | IDs, codes |
| **Templates** | `r"\\w"` ‚Üí "lorem" | Names, text |
| **Value lists** | `values=["Active", "Closed"]` | Status fields |
| **Weighted random** | `weights=[85, 10, 5]` | Realistic distributions |
| **Ranges** | `minValue=300, maxValue=850` | Numeric limits |
| **Date ranges** | `begin="2020-01-01", end="2024-12-31"` | Time periods |

### Realistic Data Patterns:

```python
# Most accounts are Active (85%), few are Closed (5%)
.withColumn(
    "account_status",
    StringType(),
    values=["Active", "Dormant", "Closed", "Frozen"],
    weights=[85, 8, 5, 2],  # 85% Active, 8% Dormant, etc.
    random=True
)

# Fraud is rare (~3%) but more common for large amounts
.withColumn(
    "fraud_flag",
    BooleanType(),
    values=[False, True],
    weights=[97, 3],  # Only 3% are fraud
    random=True
)
```

### The Data Flow:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PySpark Data Generation                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  1. DEFINE SCHEMA                                               ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ     ‚îÇ DataGenerator(rows=5000)                 ‚îÇ                ‚îÇ
‚îÇ     ‚îÇ   .withColumn("name", template="\\w")    ‚îÇ                ‚îÇ
‚îÇ     ‚îÇ   .withColumn("score", min=300, max=850) ‚îÇ                ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                         ‚îÇ                                       ‚îÇ
‚îÇ                         ‚ñº                                       ‚îÇ
‚îÇ  2. GENERATE DATA (in parallel using Spark)                    ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ     ‚îÇWorker 1 ‚îÇ ‚îÇWorker 2 ‚îÇ ‚îÇWorker 3 ‚îÇ ‚îÇWorker 4 ‚îÇ           ‚îÇ
‚îÇ     ‚îÇ 1250    ‚îÇ ‚îÇ 1250    ‚îÇ ‚îÇ 1250    ‚îÇ ‚îÇ 1250    ‚îÇ           ‚îÇ
‚îÇ     ‚îÇ rows    ‚îÇ ‚îÇ rows    ‚îÇ ‚îÇ rows    ‚îÇ ‚îÇ rows    ‚îÇ           ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ          ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ                 ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                            ‚ñº                                    ‚îÇ
‚îÇ  3. COMBINE & SAVE                                             ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ     ‚îÇ         5000 rows combined               ‚îÇ                ‚îÇ
‚îÇ     ‚îÇ              ‚îÇ                           ‚îÇ                ‚îÇ
‚îÇ     ‚îÇ              ‚ñº                           ‚îÇ                ‚îÇ
‚îÇ     ‚îÇ       PostgreSQL Table                   ‚îÇ                ‚îÇ
‚îÇ     ‚îÇ       (raw_customers)                    ‚îÇ                ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ What DBT Does For You

### DBT's Three-Layer Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     DBT TRANSFORMATION LAYERS                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  LAYER 1: STAGING (stg_)                                       ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                          ‚îÇ
‚îÇ  ‚Ä¢ Clean raw data                                               ‚îÇ
‚îÇ  ‚Ä¢ Rename columns to standard names                             ‚îÇ
‚îÇ  ‚Ä¢ Add calculated fields                                        ‚îÇ
‚îÇ  ‚Ä¢ Fix data types                                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Example: stg_customers.sql                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ SELECT                                               ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   id as customer_pk,        -- Rename for clarity   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   INITCAP(first_name),      -- Capitalize names     ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   EXTRACT(YEAR FROM AGE(date_of_birth)) as age      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ FROM raw_customers          -- Calculate age        ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                         ‚îÇ                                       ‚îÇ
‚îÇ                         ‚ñº                                       ‚îÇ
‚îÇ  LAYER 2: DIMENSIONS & FACTS (dim_, fct_)                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚îÇ
‚îÇ  ‚Ä¢ Build the star schema                                        ‚îÇ
‚îÇ  ‚Ä¢ Join related data                                            ‚îÇ
‚îÇ  ‚Ä¢ Add business logic                                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Example: dim_customers.sql                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ SELECT                                               ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   c.*,                                               ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   COUNT(a.account_id) as total_accounts,            ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   SUM(t.amount) as lifetime_value,                  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   CASE WHEN last_txn > 30 THEN 'Inactive'           ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ        ELSE 'Active' END as status                  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ FROM stg_customers c                                 ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ JOIN stg_accounts a ...                              ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                         ‚îÇ                                       ‚îÇ
‚îÇ                         ‚ñº                                       ‚îÇ
‚îÇ  LAYER 3: ANALYTICS                                            ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                          ‚îÇ
‚îÇ  ‚Ä¢ Business intelligence views                                  ‚îÇ
‚îÇ  ‚Ä¢ Ready-to-use reports                                         ‚îÇ
‚îÇ  ‚Ä¢ KPIs and metrics                                             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Example: fraud_detection_report.sql                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ SELECT                                               ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   customer_id,                                       ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   COUNT(fraud_events) as fraud_count,               ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   SUM(fraud_amount) as total_fraud_amount,          ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   CASE WHEN fraud_count > 3 THEN 'High Risk'        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ        ELSE 'Normal' END as risk_level              ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ FROM fct_fraud_events                                ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ GROUP BY customer_id                                 ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### DBT Commands Explained

| Command | What It Does | When to Use |
|---------|-------------|-------------|
| `dbt run` | Builds all models (tables/views) | Run your transformations |
| `dbt test` | Validates data quality | Check for errors |
| `dbt docs generate` | Creates documentation | Before sharing |
| `dbt docs serve` | Opens docs in browser | To explore lineage |
| `dbt debug` | Tests database connection | When things don't work |

### DBT Model Types

```sql
-- VIEW (default): Runs every time you query
{{ config(materialized='view') }}
SELECT * FROM raw_customers

-- TABLE: Stores data, faster queries
{{ config(materialized='table') }}
SELECT * FROM raw_customers

-- INCREMENTAL: Only processes new data (efficient!)
{{ config(materialized='incremental', unique_key='id') }}
SELECT * FROM raw_transactions
{% if is_incremental() %}
WHERE timestamp > (SELECT MAX(timestamp) FROM {{ this }})
{% endif %}
```

### DBT's Ref Function

The magic `{{ ref() }}` function handles dependencies automatically:

```sql
-- DBT knows to build dim_customers BEFORE this model
SELECT * 
FROM {{ ref('dim_customers') }}  -- Not 'dim_customers' directly!
JOIN {{ ref('fct_transactions') }}
```

```
DBT builds in correct order:
1. stg_customers (no dependencies)
2. stg_accounts (no dependencies)  
3. dim_customers (depends on stg_customers)
4. fct_transactions (depends on stg_*)
5. fraud_detection_report (depends on fct_*)
```

---

## ‚≠ê The Star Schema Pattern

### What is a Star Schema?

A **star schema** organizes data with:
- **Fact tables** (center) - Events, transactions, measurements
- **Dimension tables** (points of star) - Descriptive attributes

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇdim_customers‚îÇ
                    ‚îÇ  - name     ‚îÇ
                    ‚îÇ  - segment  ‚îÇ
                    ‚îÇ  - score    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ dim_date    ‚îÇ       ‚îÇ        ‚îÇdim_accounts ‚îÇ
    ‚îÇ - year      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ - type      ‚îÇ
    ‚îÇ - month     ‚îÇ       ‚îÇ        ‚îÇ - balance   ‚îÇ
    ‚îÇ - day       ‚îÇ       ‚îÇ        ‚îÇ - status    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ fct_transactions ‚îÇ  ‚Üê FACT TABLE (center)
                 ‚îÇ - amount         ‚îÇ
                 ‚îÇ - timestamp      ‚îÇ
                 ‚îÇ - customer_key   ‚îÇ  ‚Üê Links to dimensions
                 ‚îÇ - account_key    ‚îÇ
                 ‚îÇ - date_key       ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ dim_cards   ‚îÇ       ‚îÇ        ‚îÇdim_branches ‚îÇ
    ‚îÇ - type      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ - city      ‚îÇ
    ‚îÇ - network   ‚îÇ                ‚îÇ - state     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why Use Star Schema?

| Benefit | Explanation |
|---------|-------------|
| **Fast queries** | Simple JOINs, optimized for analytics |
| **Easy to understand** | Business users can navigate easily |
| **Flexible analysis** | Slice data by any dimension |
| **Scalable** | Add new dimensions without changing facts |

### Our Star Schema Tables

**Dimension Tables (describe things):**
| Table | Description | Key Attributes |
|-------|-------------|----------------|
| `dim_customers` | Who are our customers? | name, segment, credit_score |
| `dim_accounts` | What accounts exist? | type, balance, status |
| `dim_branches` | Where are our branches? | city, state, performance |
| `dim_cards` | What cards are issued? | type, network, utilization |
| `dim_loans` | What loans are active? | type, amount, status |
| `dim_date` | When did things happen? | year, month, weekday |

**Fact Tables (measure things):**
| Table | Description | Key Metrics |
|-------|-------------|-------------|
| `fct_transactions` | Every transaction | amount, status, fraud_flag |
| `fct_fraud_events` | Suspected fraud | risk_level, indicator |
| `fct_daily_account_balance` | Daily balances | inflow, outflow |
| `fct_customer_daily_summary` | Daily activity | transaction_count |

---

## üöÄ Quick Start Guide

### Prerequisites

```bash
# 1. Install Python 3.8+ and Java 11+
python --version  # Should be 3.8+
java -version     # Should be 11+

# 2. Start PostgreSQL (using Docker)
docker-compose up -d

# 3. Activate virtual environment
source env/bin/activate  # Mac/Linux
# or: env\Scripts\activate  # Windows

# 4. Install dependencies
pip install -r requirements.txt
```

### Run the Pipeline

```bash
cd generateData

# Option 1: Full pipeline (recommended first time)
python main.py --job full

# Option 2: Step by step
python main.py --job generate    # Create fake data
python main.py --job transform   # Run DBT models
python main.py --job docs        # Generate documentation
```

### Expected Output

```
============================================================
üè¶ BANKING DATA PIPELINE
============================================================

üè¶ STEP 1: Generating Banking Data with PySpark
   ‚úÖ Written 50 rows to raw_branches
   ‚úÖ Written 5000 rows to raw_customers
   ‚úÖ Written 8000 rows to raw_accounts
   ‚úÖ Written 6000 rows to raw_cards
   ‚úÖ Written 2000 rows to raw_loans
   ‚úÖ Written 50000 rows to raw_transactions

üîÑ STEP 2: Running DBT Transformations
   ‚úÖ 26 of 26 models completed successfully

‚úÖ PIPELINE COMPLETED SUCCESSFULLY
   Total time: ~15 seconds
```

---

## üìÅ Project Structure

```
datapipeline/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ üìÑ .gitignore               # Files to ignore in git
‚îÇ
‚îú‚îÄ‚îÄ üìÅ generateData/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py              # üöÄ Main entry point
‚îÇ   ‚îÇ                            # Run with: python main.py --job full
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ .env                  # Environment variables (DB credentials)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ config.py         # Loads settings from .env
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ DataCore.py       # Spark session manager
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ job/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ banking_generator.py  # üè≠ Data generation logic
‚îÇ   ‚îÇ                                 # Creates all 6 raw tables
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ dbt_transform/        # üîÑ DBT project
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ dbt_project.yml   # DBT configuration
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ üìÅ models/
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ üìÅ staging/      # Layer 1: Clean raw data
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ sources.yml  # Define raw table sources
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ stg_customers.sql
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ stg_accounts.sql
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ stg_transactions.sql
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ stg_cards.sql
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ stg_loans.sql
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ stg_branches.sql
‚îÇ           ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ üìÅ marts/        # Layer 2 & 3: Business models
‚îÇ               ‚îÇ
‚îÇ               ‚îú‚îÄ‚îÄ üìÅ dimensions/   # Star schema dimensions
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ dim_customers.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ dim_accounts.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ dim_branches.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ dim_cards.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ dim_loans.sql
‚îÇ               ‚îÇ   ‚îî‚îÄ‚îÄ dim_date.sql
‚îÇ               ‚îÇ
‚îÇ               ‚îú‚îÄ‚îÄ üìÅ facts/        # Star schema facts
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ fct_transactions.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ fct_fraud_events.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ fct_daily_account_balance.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ fct_customer_daily_summary.sql
‚îÇ               ‚îÇ   ‚îú‚îÄ‚îÄ fct_loan_payments.sql
‚îÇ               ‚îÇ   ‚îî‚îÄ‚îÄ fct_card_transactions.sql
‚îÇ               ‚îÇ
‚îÇ               ‚îî‚îÄ‚îÄ üìÅ analytics/    # Business intelligence
‚îÇ                   ‚îú‚îÄ‚îÄ fraud_detection_report.sql
‚îÇ                   ‚îú‚îÄ‚îÄ customer_segmentation.sql
‚îÇ                   ‚îú‚îÄ‚îÄ credit_risk_assessment.sql
‚îÇ                   ‚îú‚îÄ‚îÄ transaction_trends.sql
‚îÇ                   ‚îú‚îÄ‚îÄ account_health_dashboard.sql
‚îÇ                   ‚îú‚îÄ‚îÄ loan_portfolio_analysis.sql
‚îÇ                   ‚îú‚îÄ‚îÄ branch_performance.sql
‚îÇ                   ‚îî‚îÄ‚îÄ network_performance_analysis.sql
```

---

## üîç Sample Queries

### Connect to PostgreSQL

```bash
# Using psql
docker exec -it my-postgres-container psql -U postgres -d device_db

# Or use any SQL client (DBeaver, pgAdmin, etc.)
# Host: localhost, Port: 5432, Database: device_db
```

### 1. Find Fraud Patterns

```sql
-- Top customers with suspicious activity
SELECT 
    full_name,
    total_fraud_attempts,
    total_fraud_amount,
    fraud_risk_level,
    recommended_action
FROM fraud_detection_report
WHERE fraud_risk_level = 'High Risk'
ORDER BY total_fraud_amount DESC
LIMIT 10;
```

### 2. Customer Segmentation

```sql
-- How are customers distributed?
SELECT 
    customer_lifecycle_segment,
    COUNT(*) as customer_count,
    ROUND(AVG(last_30d_volume)::numeric, 2) as avg_monthly_volume
FROM customer_segmentation
GROUP BY customer_lifecycle_segment
ORDER BY customer_count DESC;
```

### 3. Credit Risk Overview

```sql
-- Customers who might default
SELECT 
    full_name,
    credit_score,
    total_loans,
    debt_to_income_ratio,
    overall_risk_level,
    credit_recommendation
FROM credit_risk_assessment
WHERE overall_risk_level IN ('High Risk', 'Medium-High Risk')
ORDER BY debt_to_income_ratio DESC;
```

### 4. Transaction Trends

```sql
-- Daily transaction patterns
SELECT 
    day_name,
    SUM(total_transactions) as total_txns,
    ROUND(AVG(total_volume)::numeric, 2) as avg_volume,
    ROUND(AVG(fraud_rate_percent)::numeric, 2) as avg_fraud_rate
FROM transaction_trends
GROUP BY day_name
ORDER BY total_txns DESC;
```

### 5. Branch Performance

```sql
-- Best performing branches
SELECT 
    branch_name,
    city,
    total_customers,
    total_deposits,
    performance_tier
FROM branch_performance
ORDER BY total_deposits DESC
LIMIT 10;
```

---

## üêõ Troubleshooting

### Common Issues

| Error | Cause | Solution |
|-------|-------|----------|
| `PyArrow not installed` | Missing dependency | `pip install pyarrow>=17.0.0` |
| `Database does not exist` | DB not created | `docker exec my-postgres-container psql -U postgres -c "CREATE DATABASE device_db;"` |
| `ROUND function error` | PostgreSQL type issue | Cast to numeric: `ROUND(value::numeric, 2)` |
| `Connection refused` | PostgreSQL not running | `docker-compose up -d` |
| `JDBC driver not found` | JAR path wrong | Check `JAR_PATH` in `.env` |

### Debug Commands

```bash
# Check Docker is running
docker ps

# Check DBT connection
cd dbt_transform
dbt debug

# See what tables exist
docker exec my-postgres-container psql -U postgres -d device_db -c "\dt"

# Check data counts
docker exec my-postgres-container psql -U postgres -d device_db -c "
SELECT 
    'raw_customers' as table_name, COUNT(*) FROM raw_customers
UNION ALL
SELECT 'raw_transactions', COUNT(*) FROM raw_transactions;
"
```

---

## üìö Learning Resources

### For Beginners

- üìñ [DBT Fundamentals Course](https://courses.getdbt.com/courses/fundamentals) - Free, excellent
- üìñ [PySpark Tutorial](https://spark.apache.org/docs/latest/api/python/getting_started/index.html)
- üìñ [Star Schema Basics](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)

### Documentation

- [DBT Docs](https://docs.getdbt.com)
- [PySpark API](https://spark.apache.org/docs/latest/api/python/)
- [PostgreSQL Manual](https://www.postgresql.org/docs/)

---

## üìä Data Summary

| Layer | Table Count | Description |
|-------|-------------|-------------|
| **Raw** | 6 tables | PySpark-generated data |
| **Staging** | 6 views | Cleaned and standardized |
| **Dimensions** | 6 tables | Star schema descriptors |
| **Facts** | 6 tables | Transaction and event data |
| **Analytics** | 8 views | Business intelligence |
| **Total** | **32 models** | Complete data warehouse |

---

## üéì Key Concepts Glossary

| Term | Definition |
|------|------------|
| **ETL** | Extract, Transform, Load - moving and processing data |
| **Data Warehouse** | Central repository for analysis-ready data |
| **Star Schema** | Fact tables surrounded by dimension tables |
| **Dimension** | Descriptive attributes (who, what, where, when) |
| **Fact** | Measurable events (transactions, amounts) |
| **Staging** | Intermediate layer for data cleaning |
| **Incremental** | Processing only new/changed data |
| **Materialization** | How DBT stores results (view, table, incremental) |

---

**Built with ‚ù§Ô∏è for learning modern data engineering**

*Questions? Check the troubleshooting section or open an issue!*
